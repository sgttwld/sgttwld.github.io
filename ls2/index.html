<!DOCTYPE html>
<html>
  <head>
    <title>Learning Systems 2</title>
    <meta name="keywords" content="machine learning, decision-making , mathematics, python, tensorflow">
    <meta name="author" content="Sebastian Gottwald">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">
    <link rel="apple-touch-icon" sizes="57x57" href="../static/icons/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="../static/icons/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="../static/icons/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="../static/icons/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="../static/icons/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="../static/icons/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="../static/icons/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="../static/icons/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="../static/icons/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192"  href="../static/icons/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../static/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="../static/icons/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../static/icons/favicon-16x16.png">
    <link rel="manifest" href="../manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="../static/icons/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

    <link rel="dns-prefetch" href="//fonts.googleapis.com/">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../static/style.css?version=5">

  </head>

  <body>
    <div id="wrapper">



      <div id="content">

        <!-- <div class="quote">I learned very early the difference between knowing the name of something and knowing something. (Richard P. Feynman)
        </div> -->

        
        <div class='quick_info'>
          <h1>Learning Systems II</h1> 
          <div class='short_description'>
            Sebastian Gottwald, Ulm University
          </div>
        </div>


        <h2>Instance-based Learning <span class='links'>(<a target="_blank" href="slides/kernels.html">slides</a>, <a target="_blank" href="slides/pdfs/LS2_InstanceBasedLearning.pdf">pdf</a>, <a target="_blank" href="https://github.com/sgttwld/LearningSystems2/tree/master/InstanceBasedLearning">code</a>)</span></h2>
        
        <ul>
        <li><a href="slides/kernels.html#/1"><b>Motivation</b></a>
          <ul class='sublist'>
            <li>Basic Idea behind instance-based learning (<a href="slides/kernels.html#/1/1">slide 4</a>)</li>
            <li>Example: Kernel regression (<a href="slides/kernels.html#/1/2">slide 5</a>)</li>
            <li>Inner product kernels (<a href="slides/kernels.html#/1/3">slide 6</a>)</li>
          </ul>
        </li>
        <li><a href="slides/kernels.html#/2"><b>Reproducing Kernel Hilbert Spaces</b></a>
          <ul class='sublist'>
            <li>Vector spaces (<a href="slides/kernels.html#/2/1">slide 12</a>)</li>
            <li>Inner product spaces (<a href="slides/kernels.html#/2/2">slide 13</a>)</li>
            <li>Induced norm (<a href="slides/kernels.html#/2/3">slide 14</a>)</li>
            <li>Cauchy-Schwarz inequality (<a href="slides/kernels.html#/2/4">slide 15</a>)</li>
            <li>Hilbert spaces (<a href="slides/kernels.html#/2/5">slide 16</a>)</li>
            <li>Dual spaces (<a href="slides/kernels.html#/2/6">slide 17</a>)</li>
            <li>Riesz representation theorem (<a href="slides/kernels.html#/2/7">slide 18</a>)</li>
            <li>Evaluation functionals (<a href="slides/kernels.html#/2/8">slide 19</a>)</li>
            <li>Reproducing kernel Hilbert spaces (<a href="slides/kernels.html#/2/9">slide 20</a>)</li>
          </ul>
        </li>
        <li><a href="slides/kernels.html#/3"><b>Kernel machines</b></a>
          <ul class='sublist'>
            <li>Linear Support Vector Machine: Definition (<a href="slides/kernels.html#/3/1">slide 26</a>)</li>
            <li>Constrained to unconstrained optimization (<a href="slides/kernels.html#/3/2">slide 27</a>)</li>
            <li>Duality in constrained optimization (<a href="slides/kernels.html#/3/3">slide 28</a>)</li>
            <li>KKT conditions (<a href="slides/kernels.html#/3/4">slide 29</a>)</li>
            <li>Linear SVM: Dual Problem (<a href="slides/kernels.html#/3/5">slide 30</a>)</li>
            <li>Linear SVM: Solution (<a href="slides/kernels.html#/3/6">slide 31</a>)</li>
            <li>Nonlinear SVM (<a href="slides/kernels.html#/3/7">slide 32</a>)</li>
            <li>Extensions (<a href="slides/kernels.html#/3/8">slide 33</a>)</li>
            <li>Kernel trick (<a href="slides/kernels.html#/3/9">slide 34</a>)</li>
            <li>Other methods (<a href="slides/kernels.html#/3/10">slide 35</a>)</li>
          </ul>

        </li>
        </ul>  
        

        <h2>Learning in Graphical Models <span class='links'>(<a target="_blank" href="slides/graphs.html">slides</a>, <a target="_blank" href="slides/pdfs/LS2_LearningInGraphicalModels.pdf">pdf</a>, <a target="_blank" href="https://github.com/sgttwld/LearningSystems2/tree/master/GraphicalModels">code</a>)</span></h2>


        <ul>
        <li><a href="slides/graphs.html#/1"><b>Recap: Probability calculus</b></a>
          <ul class='sublist'>
            <li>Probability distributions (<a href="slides/graphs.html#/1/1">slide 4</a>)</li>
            <li>Probability measure (<a href="slides/graphs.html#/1/3">slide 6</a>)</li>
            <li>Random experiments (<a href="slides/graphs.html#/1/4">slide 7</a>)</li>
            <li>Events (<a href="slides/graphs.html#/1/5">slide 8</a>)</li>
            <li>Probability space (<a href="slides/graphs.html#/1/6">slide 9</a>)</li>
            <li>Random Variables (<a href="slides/graphs.html#/1/7">slide 10</a>)</li>
            <li>Expectation (<a href="slides/graphs.html#/1/11">slide 14</a>)</li>
            <li>Joint distributions (<a href="slides/graphs.html#/1/14">slide 17</a>)</li>
            <li>Marginalization (<a href="slides/graphs.html#/1/15">slide 18</a>)</li>
            <li>Conditioning (<a href="slides/graphs.html#/1/16">slide 19</a>)</li>
            <li>Canonical factorization of the joint (<a href="slides/graphs.html#/1/18">slide 21</a>)</li>
            <li>Statistical independence (<a href="slides/graphs.html#/1/19">slide 22</a>)</li>
          </ul>
        </li>


        <li><a href="slides/graphs.html#/2"><b>Directed graphs</b></a>
          <ul class='sublist'>
            <li>Graphs (<a href="slides/graphs.html#/2/1">slide 25</a>)</li>
            <li>Representing joints graphically (<a href="slides/graphs.html#/2/2">slide 26</a>)</li>
            <li>Conditional independence (d-separation) (<a href="slides/graphs.html#/2/5">slide 29</a>)</li>
            <li>Markov property (<a href="slides/graphs.html#/2/10">slide 34</a>)</li>
            <li>Inference (<a href="slides/graphs.html#/2/14">slide 38</a>)</li>
            <li>Maximum likelihood (<a href="slides/graphs.html#/2/15">slide 39</a>)</li>
            <li>Bayesian inference (<a href="slides/graphs.html#/2/16">slide 40</a>)</li>
            <li>Variational inference (<a href="slides/graphs.html#/2/17">slide 41</a>)</li>
            <li>General free energy minimization (<a href="slides/graphs.html#/2/19">slide 43</a>)</li>
            <li>Approximate and iterative inference (<a href="slides/graphs.html#/2/23">slide 47</a>)</li>
          </ul>
        </li>
        <li><a href="slides/graphs.html#/3"><b>Undirected graphs</b></a>
          <ul class='sublist'>
            <li>Cliques (<a href="slides/graphs.html#/3/1">slide 51</a>)</li>
            <li>Representing joints by undirected graphs (<a href="slides/graphs.html#/3/2">slide 52</a>)</li>
            <li>Directed to undirected graphs (<a href="slides/graphs.html#/3/3">slide 53</a>)</li>
            <li>Conditional independence for undirected graphs (<a href="slides/graphs.html#/3/7">slide 57</a>)</li>
            <li>Exponential representation of potentials (<a href="slides/graphs.html#/3/11">slide 61</a>)</li>
            <li>Boltzmann machines (<a href="slides/graphs.html#/3/12">slide 62</a>)</li>
            <li>Restricted Boltzmann machines (<a href="slides/graphs.html#/3/20">slide 70</a>)</li>
          </ul>
        <li><a href="slides/graphs.html#/4"><b>Message passing</b></a>
          <ul class='sublist'>
            <li>Inference on a chain (<a href="slides/graphs.html#/4/1">slide 78</a>)</li>
            <li>Factor graphs (<a href="slides/graphs.html#/4/6">slide 83</a>)</li>
            <li>Inference in factor graphs (<a href="slides/graphs.html#/4/7">slide 84</a>)</li>
            <li>Motivating the sum-product algorithm from examples (<a href="slides/graphs.html#/4/8">slide 85</a>)</li>
            <li>The sum-product algorithm (<a href="slides/graphs.html#/4/12">slide 89</a>)</li>
            <li>Learning in HMMs (<a href="slides/graphs.html#/4/19">slide 96</a>)</li>
            <li>Inference over hidden variables using message passing (<a href="slides/graphs.html#/4/22">slide 99</a>)</li>
          </ul>
        </li>
        </ul>  
        


        
        <h2>Learning with Attention <span class='links'>(<a target="_blank" href="slides/attention.html">slides</a>, <a target="_blank" href="slides/pdfs/LS2_LearningWithAttention.pdf">pdf</a>, <a target="_blank" href="">code</a>)</span></h2>
 
        

        <ul>
        <li><a href="slides/attention.html#/1"><b>Recurrent neural networks</b></a>
          <ul class='sublist'>
            <li>Recap: Learning feed-forward artificial neural networks (<a href="slides/attention.html#/1/1">slide 4</a>)</li>
            <li>Sequential data (<a href="slides/attention.html#/1/3">slide 6</a>)</li>
            <li>n-Gram models (<a href="slides/attention.html#/1/4">slide 7</a>)</li>
            <li>Basic structure of RNNs (<a href="slides/attention.html#/1/5">slide 8</a>)</li>
            <li>Single-layer RNNs (<a href="slides/attention.html#/1/6">slide 9</a>)</li>
            <li>Multilayer RNNs (<a href="slides/attention.html#/1/7">slide 10</a>)</li>
            <li>Example applications (<a href="slides/attention.html#/1/8">slide 11</a>)</li>
            <li>Variants of RNNs (<a href="slides/attention.html#/1/9">slide 12</a>)</li>
            <li>Exploding and vanishing gradients (<a href="slides/attention.html#/1/10">slide 13</a>)</li>
          </ul>
        </li>

        <li><a href="slides/attention.html#/2"><b>Encoder-decoder architectures</b></a>
          <ul class='sublist'>
            <li>Sequence to sequence models (<a href="slides/attention.html#/2/1">slide 16</a>)</li>
            <li>Basic encoder-decoder architecture (<a href="slides/attention.html#/2/2">slide 17</a>)</li>
            <li>Refinements of encoder-decoder architectures (<a href="slides/attention.html#/2/4">slide 19</a>)</li>
            <li>Bahdanau et al. (2015): First encoder-decoder RNN with attention (<a href="slides/attention.html#/2/5">slide 20</a>)</li>
            <li>Summary: Attention in RNNs (<a href="slides/attention.html#/2/7">slide 22</a>)</li>
          </ul>
        </li>

        <li><a href="slides/attention.html#/3"><b>Transformers</b></a>
          <ul class='sublist'>
            <li>Dot-product attention (<a href="slides/attention.html#/3/1">slide 26</a>)</li>
            <li>Query-key-value mechanism (<a href="slides/attention.html#/3/2">slide 27</a>)</li>
            <li>Vaswani et al. (2017): "Attention is all you need" (<a href="slides/attention.html#/3/4">slide 29</a>)</li>
            <li>Self attention (<a href="slides/attention.html#/3/5">slide 30</a>)</li>
            <li>Encoder-decoder attention (<a href="slides/attention.html#/3/7">slide 32</a>)</li>
            <li>Masked self attention (<a href="slides/attention.html#/3/8">slide 33</a>)</li>
            <li>Teacher forcing (<a href="slides/attention.html#/3/10">slide 35</a>)</li>
            <li>Additional techincal details about transformers (<a href="slides/attention.html#/3/12">slide 37</a>)</li>
            <li>Applications to other tasks (<a href="slides/attention.html#/3/14">slide 39</a>)</li>
            <li>Quadratic costs (<a href="slides/attention.html#/3/15">slide 40</a>)</li>
          </ul>
        </li>

        <li><a href="slides/attention.html#/4"><b>Vision Transformers</b></a>
          <ul class='sublist'>
            <li>Dosovitskiy et al. (2021): Vision transformer, ViT (<a href="slides/attention.html#/4/1">slide 44</a>)</li>
            <li>Touvron et al. (2021): Data-efficient image transformers, DeiT (<a href="slides/attention.html#/4/3">slide 46</a>)</li>
            <li>Most useful augmentations for image transformers (<a href="slides/attention.html#/4/4">slide 47</a>)</li>
            <li>Liu et al. (2021): Swin Transformer (<a href="slides/attention.html#/4/6">slide 49</a>)</li>
            <li>Hierarchical structure of Swin transformers (<a href="slides/attention.html#/4/7">slide 50</a>)</li>
            <li>Local windowed self-attention (<a href="slides/attention.html#/4/8">slide 51</a>)</li>
          </ul>
        </li>
      </ul>

      </div>
    </div>
  </body>
</html>
